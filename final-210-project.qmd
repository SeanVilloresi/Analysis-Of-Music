---
title: "Final 210 Project"
author: "Sean Villoresi and Ellie Kang"
format: pdf
editor: 
  markdown: 
    wrap: sentence
---

# Introduction

Music is an essential part of culture, creativity, and history. Specific songs and types of music can have great significance to groups of people and individuals alike. In America today, the music industry is both highly regarded and hotly debated. One successful song can launch an artist to the top of the charts, etching them into modern history. The importance of music and potential significance of a single song motivates the question - what makes a song successful?

## Data

Wanting to explore this question in our project, we found a dataset (Rastelli, S. (2023, February). Spotify and Youtube, Version 2.) on Kaggle with data on Spotify streams, Youtube views, and various song characteristics. There are 28 columns, and 20,719 observations. The data was collected on Februrary 7th, 2023 by extracting the data from Youtube and Spotify. Our goal is to determine and develop the best model for predicting the success of a song based on the number of streams. We chose to use streams (as opposed to Youtube views) as our outcome variable because of inconsistencies within the data when it comes to music videos.
Some music videos were not from the artist's channel (unofficial), and we wanted to test this variable as a predictor.

Variables: We will be using streams as the model's outcome variable.
We chose the following variables as potential predictors based on their relevance to the listening experience of a song (as opposed to a more descriptive variable such as Description) *Stream*: number of streams of the song on Spotify *Energy*: a measure from 0.0 to 1.0 representing a perceptual measure (dynamic range, loudness, timbre, onset rate, general entropy) of intensity and activity. *Key*: the key the track is in measured in integers representing pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D. If no key was detected, the value is -1. *Loudness*: the overall loudness of a track in decibels (dB) *Speechiness*: a measure from 0.0 to 1.0 representing the presence of spoken words in a track. *Acousticness*: a from 0.0 to 1.0 of whether the track is acoustic. *Instrumentalness*: a measure from 0.0 to 1.0 that predicts whether a track contains no vocals. *Liveness*: a measure from 0.0 to 1.0 that detects the presence of an audience in the recording. *Valence*: a measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. *Tempo*: the overall estimated tempo of a track in beats per minute (BPM). *Duration_ms*: the duration of the track in milliseconds. *Official_video*: boolean value that indicates if the video found is the official video of the song.

We felt that the dataset had a sufficient number of both quantitative and categorical variables to test predictability. Thus, we chose not to create additional predictors. However, in our data cleaning process, we removed any observations with missing values for streams, danceability, and licensed. After removing missing values for these variables, there were no remaining observations with missing data for relevant variables as listed above.

We hypothesized that Danceability and Loudness would be significant predictors of song success measured in number of streams. We used domain knowledge and the understanding that songs that top the charts tend to be the ones that are catchy, and both danceability and loudness contribute to this. We use this model in our results section to determine if our final model is better than this model.

```{r read-data, message = F, warning = F, echo = F}
library(tidyverse)
library(tidymodels)
library(broom)
library(leaps)
library(MASS)
library(caret)
library(glmnet)
library(Stat2Data)
library(nnet)
library(lme4)
library(corrplot)
library(car)
library(reshape2)
library(gridExtra)
library(knitr)
music <- read_csv("data/Spotify_Youtube.csv")
```

```{r cleaning, message = F, warning = F, echo =F}
music$Uri=NULL
music$Url_youtube=NULL
music$Url_spotify=NULL
music$Description=NULL

music <- music[complete.cases(music$Stream), ]
music <- music[complete.cases(music$Danceability), ]
music <- music[complete.cases(music$Licensed), ]




```

## Exporatory Data Analysis

```{r distribution-streams, warning = F, echo = F, fig.width=5, fig.height=3}
ggplot(music, aes(x = Stream)) +
  geom_histogram(color = "black", fill = "lightblue", bins = 30) +
  labs(title = "Distribution of Streams",
       x = "Streams", y = "Count")
```

The distribution of the response variable Stream is skewed to the right with most of the data having lower values for Streams. This is understandable because a handful of songs are extremely popular and these outliers cause the distribution visualization to have most of the data concentrated on the left.

```{r predictor-scatterplot, message = F, warning = F, echo = F}

p1 <- ggplot(music, aes(x = Loudness, y = Stream)) +
  geom_point(color = "lightblue") +
  geom_smooth(method = "lm") +
  ggtitle("Streams vs. Loudness") +
  labs(x = "Loudness", y = "Streams")

p2 <- ggplot(music, aes(x = Danceability, y = Stream)) +
  geom_point(color = "lightblue") +
  geom_smooth(method = "lm") +
  labs(title = "Streams vs. Danceability",
       x = "Danceability", y = "Streams")

grid.arrange(p1, p2, ncol=2)
```

We visualized the relationship between Streams and all the predictors in the dataset. Danceability and Loudness showed slight positive relationships with the response variable Stream. The remaining visualizations are in the Appendix.

```{r VIF, message = F, warning = F, echo = F}
model <- lm(Stream ~ Danceability + Energy + Loudness + Speechiness + 
              Acousticness + Instrumentalness + Liveness + Valence + Tempo +
            Duration_ms + official_video,
            data = music)
kable(vif(model), caption = "VIF values")
```

The results of looking at Variance Inflation Factor are values in a range between 1 and 4. Because all the values were below 4, they did not demonstrate much multicollinearity. Thus, we chose to keep all the initial predictors when performing variable selection and did not include any interaction terms. 

```{r summary-stats, message = F, warning = F, echo = F}
summary_stats <- sapply(music[, c("Stream", "Danceability", "Loudness")], summary)


summary_df <- as.data.frame(summary_stats)

kable(summary_df, caption = "Summary Statistics of Response and Predictors of
      Interest")
```


# Methods

We will be fitting a linear model to predict streams because Stream is a non-binary numerical variable. Additionally, we are using it to represent a measure of success, and we want to determine what factors have a linear relationship with streams to determine correlations between predictors and success. Based on our EDA, we chose not to use any interaction terms.

## Variable Selection

```{r setup-for-variable-selection, message = F, warning = F, echo = F}



lm_none <- lm(Stream ~ 1 , data = music)

lm_all <- lm(Stream ~ Danceability + Energy + factor(Key) + Loudness + Speechiness +
               Acousticness + Instrumentalness + Liveness +  Valence + Tempo + 
               Duration_ms + official_video, data = music)

```

To start our modeling process, we determined which variables we would use as our "baseline", as described in our introduction above. From these, we decided the first thing we needed to do was determine if any of the variables were seen as non important/non essential, as we want to avoid overcomplicating our model. To start, we performed two variable selection methods, by using a forward and backwards stepwise method starting at our linear model for all terms, and we then proceeded to use a lasso method as well.
Between these two methods, we are fairly confident that we can determine the best variables to use.

### Step-wise Selection

```{r stepwise-selection, message = F, warning = F, echo = F}
output <- capture.output(stepAIC(lm_all, 
        scope = list(lower = lm_none, upper = lm_all),
        data = music, direction = "both"))

##Just outputs final step
#cat(tail(output, 39), sep = "\n")
```

From our stepwise method, the only variable it seemed to remove was Tempo, and even with that the change in AIC with or without tempo was fairly minimal, so it was from this point that we decided to also use lasso in order to get another perspective on the matter, knowing that stepwise functions can be very influenced by the starting model and its order of variables


### Lasso Model

```{r lasso, message = F, error = F, echo = F}
y <- music$Stream
x <- model.matrix(Stream ~ Danceability + Energy + factor(Key) + Loudness + Speechiness +
               Acousticness + Instrumentalness + Liveness +  Valence + Tempo + 
               Duration_ms + official_video,
                  data = music)

m_lasso_cv <- cv.glmnet(x, y, alpha = 1)


best_lambda <- m_lasso_cv$lambda.min
m_best <- glmnet(x, y, alpha = 1, lambda = best_lambda)

matrix_df <- as.data.frame(as.matrix(m_best$beta))


```

With LASSO however, we came to the same conclusion, as our lasso kept essentially every variable to a fairly significant coefficient. As such, we have decided to move on using all of the variables that we had started with at the beginning as predictor variables. The LASSO coefficient results are in Figure 1 of the Appendix.


```{r end-of-variable-selection, message = F, error = F, echo = F}
pretransform_model <- lm(Stream ~ Danceability + Energy + factor(Key) + Loudness 
                         + Speechiness + Acousticness + Instrumentalness 
                         + Liveness +  Valence + Tempo + Duration_ms 
                         + official_video, 
                         data = music)
```

## Linearity Assumptions and Checks for Transformations

With our variables chosen, we move on to now looking at whether our base model satisfies our assumptions required for a linear mode. We also compared our results to a transformed model where we take the log of our outcome variable Streams.

### Residual Models

```{r augment-and-residualplot, error = F, message=FALSE, echo = F, fig.width=5, fig.height=2.5}

ptmodel_aug <- augment(pretransform_model)

transform_model <- lm(log(Stream) ~ Danceability + Energy + factor(Key) + 
                        Loudness + Speechiness +Acousticness + Instrumentalness +                           Liveness +  Valence + Tempo + Duration_ms + 
                        official_video, data = music)

tmodel_aug <- augment(transform_model)


ggplot(ptmodel_aug, aes(x = .fitted, y=.resid)) +
  geom_point() +
  geom_hline(yintercept=0, color ="darkred") +
  labs(x ="Fitted Value of Streams", y = "Residual", title = "Untransformed Model") +
  theme_bw()

ggplot(tmodel_aug, aes(x = .fitted, y=.resid)) +
  geom_point() +
  geom_hline(yintercept=0, color ="darkred") +
  labs(x ="Fitted Value of Streams", y = "Residual", title = "Transformed(Log) Model") +
  theme_bw()


```

Looking at the visualizations above, we can see that the transformed model gives us a much better spread on the residual split around our red line then our untransformed model. As such, the residuals appear roughly symmetrical along the horizontal axis for our transformed plot, so we feel it safe to assume approximate linearity, specifically for our transformed model.

As it relates to constant variance, we believe that our fitted values for our transformed model seem to satisfy this condition. Other than a few outliers in our negative residual side on the right, overall we seem to see fairly constant trends with how spread out our data is.

### QQ Plots

```{r qq-plot, warning = FALSE, message = FALSE, echo = F, fig.width=5, fig.height=2.5}
ggplot(ptmodel_aug, aes(sample = .resid)) +
  stat_qq() + 
  stat_qq_line() + 
  theme_bw() + 
  labs(x = "Theoretical quantiles", 
       y = "Sample quantiles",
       title = "Untransformed Model")

ggplot(tmodel_aug, aes(sample = .resid)) +
  stat_qq() + 
  stat_qq_line() + 
  theme_bw() + 
  labs(x = "Theoretical quantiles", 
       y = "Sample quantiles",
       title = "Transformed Model")
```

Now looking at our qq plots, we see our trend continue, where our untransformed model performs quite bad as can be seen above, while our transformed model hangs much closer to our standardized line, making it a better fit. Here, we feel safe to assume normality for our transformed plot, as other then some slight deviation towards the tails, our data points hang tight to the normal line.

For our independence assumption, we believe that our data set satisfies this condition. Our data was collected all on the same day, and the streams and/or variables associated with each song should not be impacted by those of other songs.

After looking at our two graphs above, we believe that our transformed model will provide a better measurement of our data, and provide better predictions of a songs streams.

# Results

Our final model is $18.94 + .2433 * Danceability - 1.088 * Energy + (-.002984*Key_1 - .07814*Key_2 + .01597*Key_3 + .005302 * Key_4 - .01527*Key_5 + .02348*Key_6 - .05105*Key_7 + .01707*Key_8 - .07646*Key_9 - .08253*Key_{10} - .02742*Key_{11}) + .06579 * Loudness - 2.401 * Speechiness - .565*Acousticness - .4900 * Instrumentalness -.3855 * Liveness - .2931 * Valence + .001109 * Tempo + 1.507*10^{-7} * Duration_ms + .2820 * official\_videoTRUE$

The model statistics are shown in Figure 2 in the Appendix.
    
```{r model-fit, echo = F, warning = F, message = F}
set.seed(1)
cv_method <- trainControl(method = "cv", number = 10)
hypothesis_model <- train(Stream ~ Danceability + Loudness,
                   data = music, method = "lm", trControl = cv_method)

all_model <- train(Stream ~ Danceability + Energy + factor(Key) + Loudness 
                         + Speechiness + Acousticness + Instrumentalness 
                   + Liveness +  Valence + Tempo + Duration_ms + official_video,
                   data = music, method = "lm", trControl = cv_method)

final_model <- train(log(Stream) ~ Danceability + Energy + factor(Key) + 
                       Loudness + Speechiness + Acousticness + Instrumentalness 
                   + Liveness +  Valence + Tempo + Duration_ms + official_video,
                   data = music, method = "lm", trControl = cv_method)

#print(hypothesis_model)
#print(all_model)
#print(final_model)

dfmodel <- data.frame(Model = c("Hypothesis_Model", "All_Model", "Transform_Model"),
                 R_squared = c(hypothesis_model$results$Rsquared, all_model$results$Rsquared, final_model$results$Rsquared),
                 RMSE = c(hypothesis_model$results$RMSE, all_model$results$RMSE, final_model$results$RMSE))

kable(dfmodel , caption = "Model Fit Statistics")
```

Looking at our final model, we'll just look at a couple of our important hypothesized variables.

For Danceability, we have a coefficient of .2433. In this context, this means that for every 1 unit increase in the measure of danceability, we would predict an increase of .2433 in the log of the number of Streams for a given song holding all else constant. Looking at our p-value for our Danceability as well, our p-value compared to an alpha level of .05 is less then it, and as such danceability appears to be a strong predictor of streams.

For Loudness, we have a coefficient of .06579. In this context, this means that for every 1 unit increase in the measure of loudness, we would predict an increase of .2433 in the log of the number of Streams for a given song holding all else constant. Looking at our p-value for our loudness as well, our p-value compared to an alpha level of .05 is less then it, and as such loudness appears to be a strong predictor of streams.

Based on our model fit statistics, our final model with the log-transformed response variable and 12 predictors has the highest r-squared value of 0.0661358	and the lowest RMSE value of 1.592160. In addition, the un-transformed model with 12 predictors performed better than our hypothesized model using only Danceability and Loudness. Thus, we find that the model with the most predictive power included all 12 potential predictors and also required a transformation. We were correct in hypothesizing that Danceability and Loudness would be significant predictors. The final model tells us that the danceability, energy, key, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo, and duration of a song in addition to whether the music video was official, are all significant in predicting the success of a song measured in number of streams. 

# Discussion

After completing a comprehensive analysis on predictors of song streams, we found that there are many answers to our research question - what makes a song successful? We initially considered the danceability and loudness of the song as significant contributors to the significance of a song. We found that both of these predictors are statistically significant but adding more predictors strengthens the final model's predictability power, as measured by RMSE and r-squared values. In the context of the music industry, our hypothesis means that artist's should consider the danceability and loudness of their songs if number of streams is important to them. Further, to maximize streams, they should consider all 12 predictors we used as characteristics in their song. We find that songs with higher danceability, energy, acoustic, valence, and loudness levels tend to get more streams. Additionally, specific keys, the presence of an audience, less spoken words, faster tempos, longer songs, and an indication of an official music video also lead to higher streams.

One limitation of the data we used is that it only represents one day of data. Our conclusions are only directly applicable to the songs on the day the data was collected. Another limitation is that it only includes Spotify streams, so our analysis does not include conclusions based on data for other streaming services. Also, we did not standardize the units of our predictors to explore the different levels of significance of their respective impacts on streams. Had we done this, we could make conclusions about which song characteristics were more important than others for predicting success. Finally, we were unable to use the YouTube data due to inconsistencies with the observations.

In the future, data beyond a singular day should be used for analysis, in addition to data from other streaming services. To test our model further, we could apply it to data from other streaming services to see how it performs. Future work should include exploration of the significance of predictors relative to each other and incorporation of the data from YouTube.

\newpage
# Appendix

## Figure 1

```{r appendix-stuff, message = F, warning = F, echo = F}
kable(matrix_df , caption = "Lasso Coefficients")
```

## Figure 2

```{r results, message = F, echo = F, warning = F}
summary(transform_model)
```



