---
title: "Final 210 Project"
author: "Sean Villoresi and Ellie Kang"
format: pdf
---

# Introduction

Music is an essential part of culture, creativity, and history. Specific songs and types of music can have great significance to groups of people and individuals alike. In America today, the music industry is both highly regarded and hotly debated. One successful song can launch an artist to the top of the charts, etching them into modern history. The importance of music and potential significance of a single song motivates the question - what makes a song successful?

Wanting to explore this question in our project, we found a dataset1 on Kaggle with data on Spotify streams, Youtube views, and various song characteristics. There are 28 columns, and 20,719 observations. The data was collected on Februrary 7th, 2023 by extracting the data from Youtube and Spotify. Our goal is to determine and develop the best model for predicting the success of a song based on the number of streams. We chose to use streams (as opposed to Youtube views) as our outcome variable because of inconsistencies within the data when it comes to music videos. Some music videos were not from the artist’s channel (unofficial), and we wanted to test this variable as a predictor.

Variables: We will be using streams as the model’s outcome variable. We chose the following variables as potential predictors based on their relevance to the listening experience of a song (as opposed to a more descriptive variable such as Description)
Stream: number of streams of the song on Spotify
Energy:  a measure from 0.0 to 1.0 representing a perceptual measure (dynamic range, loudness, timbre, onset rate, general entropy) of intensity and activity.
Key: the key the track is in measured in ntegers representing pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D. If no key was detected, the value is -1.
Loudness: the overall loudness of a track in decibels (dB)
Speechiness: a measure from 0.0 to 1.0 representing the presence of spoken words in a track. 
Acousticness: a from 0.0 to 1.0 of whether the track is acoustic.
Instrumentalness: a measure from 0.0 to 1.0 that predicts whether a track contains no vocals. 
Liveness: a measure from 0.0 to 1.0 that detects the presence of an audience in the recording.
Valence: a measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track.
Tempo: the overall estimated tempo of a track in beats per minute (BPM).
Duration_ms: the duration of the track in milliseconds.
Official_video: boolean value that indicates if the video found is the official video of the song.
We felt that the dataset had a sufficient number of both quantitative and categorical variables to test predictability. Thus, we chose not to create additional predictors. However, in our data cleaning process, we removed any observations with missing values for streams, danceability, and licensed. After removing missing values for these variables, there were no remaining observations with missing data for relevant variables as listed above.



```{r read-data, message = F, warning = F, echo = F}
library(tidyverse)
library(tidymodels)
library(broom)
library(leaps)
library(MASS)
library(caret)
library(glmnet)
library(Stat2Data)
library(nnet)
library(lme4)
music <- read_csv("data/Spotify_Youtube.csv")
```



```{r cleaning, message = F, warning = F, echo =F}
music$Uri=NULL
music$Url_youtube=NULL
music$Url_spotify=NULL
music$Description=NULL

music <- music[complete.cases(music$Stream), ]
music <- music[complete.cases(music$Danceability), ]
music <- music[complete.cases(music$Licensed), ]




```

# Variable Selection

```{r setup-for-variable-selection, message = F, warning = F, echo = F}



lm_none <- lm(Stream ~ 1 , data = music)

lm_all <- lm(Stream ~ Danceability + Energy + factor(Key) + Loudness + Speechiness +
               Acousticness + Instrumentalness + Liveness +  Valence + Tempo + 
               Duration_ms + official_video, data = music)

```

## Step-wise Selection
```{r stepwise-selection, message = F, warning = F, echo = F}
output <- capture.output(stepAIC(lm_all, 
        scope = list(lower = lm_none, upper = lm_all),
        data = music, direction = "both"))

##Just outputs final step
cat(tail(output, 39), sep = "\n")


```
## Lasso Model
```{r lasso, message = F, error = F, echo = F}
y <- music$Stream
x <- model.matrix(Stream ~ Danceability + Energy + factor(Key) + Loudness + Speechiness +
               Acousticness + Instrumentalness + Liveness +  Valence + Tempo + 
               Duration_ms + official_video,
                  data = music)

m_lasso_cv <- cv.glmnet(x, y, alpha = 1)


best_lambda <- m_lasso_cv$lambda.min
m_best <- glmnet(x, y, alpha = 1, lambda = best_lambda)
m_best$beta 

```

```{r end-of-variable-selection, message = F, error = F, echo = F}
## CHANGE LATER AFTER WE ACTUALLY CHOOSE MODELS FROM ABOVE< CURRENTLY A PLACE HOLDER.
pretransform_model <- lm(Stream ~ Danceability + Energy + factor(Key) + Loudness 
                         + Speechiness +
               Acousticness + Instrumentalness + Liveness +  Valence + Tempo + 
               Duration_ms + official_video, data = music)



```

# Linearity Assumptions and Checks for Transformations

## Residual Models

```{r augment-and-residualplot, error = F, message=FALSE, echo = F}

ptmodel_aug <- augment(pretransform_model)

transform_model <- lm(log(Stream) ~ Danceability + Energy + factor(Key) + Loudness + 
                        Speechiness +Acousticness + Instrumentalness + Liveness 
                      +  Valence + Tempo + Duration_ms + official_video, 
                      data = music)

tmodel_aug <- augment(transform_model)


ggplot(ptmodel_aug, aes(x = .fitted, y=.resid)) +
  geom_point() +
  geom_hline(yintercept=0, color ="darkred") +
  labs(x ="Fitted Value of Streams", y = "Residual") +
  theme_bw()

ggplot(tmodel_aug, aes(x = .fitted, y=.resid)) +
  geom_point() +
  geom_hline(yintercept=0, color ="darkred") +
  labs(x ="Fitted Value of Streams", y = "Residual") +
  theme_bw()


```

Looking at the visualizations above, we can see that the transformed model gives us a much better spread on the residual split around our red line then our untransformed model. As such, the residuals appear roughly symmetrical along the horizontal axis for our transformed plot, so we feel it safe to assume approximate linearity. 

## QQ Plots

``` {r qq-plot, warning = FALSE, message = FALSE, echo = F}
ggplot(ptmodel_aug, aes(sample = .resid)) +
  stat_qq() + 
  stat_qq_line() + 
  theme_bw() + 
  labs(x = "Theoretical quantiles", 
       y = "Sample quantiles")

ggplot(tmodel_aug, aes(sample = .resid)) +
  stat_qq() + 
  stat_qq_line() + 
  theme_bw() + 
  labs(x = "Theoretical quantiles", 
       y = "Sample quantiles")

```
















